{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jkenney\\appdata\\local\\continuum\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "c:\\users\\jkenney\\appdata\\local\\continuum\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapefile\n",
    "import time\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Import data and check the shape of train and test set.\n",
    "We want to determine how to split out the train set into train and dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of train data', (878049, 9))\n",
      "('Shape of test data', (884262, 7))\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "ff = \"./Data/train.csv\" # you will need to edit this directory\n",
    "with open(ff, 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    train_data = list(reader)\n",
    "ff = \"./Data/test.csv\" # you will need to edit this directory\n",
    "with open(ff, 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_data = list(reader)\n",
    "    \n",
    "#Convert to pandas data frame for better analysis\n",
    "train_data_full = pd.DataFrame(train_data[1:], columns = train_data[0])\n",
    "test_data = pd.DataFrame(test_data[1:], columns = test_data[0])\n",
    "print (\"Shape of train data\", train_data_full.shape)\n",
    "print (\"Shape of test data\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run basic totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Top 5 crimes\n",
      "Category\n",
      "LARCENY/THEFT     174900\n",
      "OTHER OFFENSES    126182\n",
      "NON-CRIMINAL       92304\n",
      "ASSAULT            76876\n",
      "DRUG/NARCOTIC      53971\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: Lowest 5 crimes\n",
      "Category\n",
      "TREA                           6\n",
      "PORNOGRAPHY/OBSCENE MAT       22\n",
      "GAMBLING                     146\n",
      "SEX OFFENSES NON FORCIBLE    148\n",
      "EXTORTION                    256\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: All Category Counts\n",
      "Category\n",
      "LARCENY/THEFT                  174900\n",
      "OTHER OFFENSES                 126182\n",
      "NON-CRIMINAL                    92304\n",
      "ASSAULT                         76876\n",
      "DRUG/NARCOTIC                   53971\n",
      "VEHICLE THEFT                   53781\n",
      "VANDALISM                       44725\n",
      "WARRANTS                        42214\n",
      "BURGLARY                        36755\n",
      "SUSPICIOUS OCC                  31414\n",
      "MISSING PERSON                  25989\n",
      "ROBBERY                         23000\n",
      "FRAUD                           16679\n",
      "FORGERY/COUNTERFEITING          10609\n",
      "SECONDARY CODES                  9985\n",
      "WEAPON LAWS                      8555\n",
      "PROSTITUTION                     7484\n",
      "TRESPASS                         7326\n",
      "STOLEN PROPERTY                  4540\n",
      "SEX OFFENSES FORCIBLE            4388\n",
      "DISORDERLY CONDUCT               4320\n",
      "DRUNKENNESS                      4280\n",
      "RECOVERED VEHICLE                3138\n",
      "KIDNAPPING                       2341\n",
      "DRIVING UNDER THE INFLUENCE      2268\n",
      "RUNAWAY                          1946\n",
      "LIQUOR LAWS                      1903\n",
      "ARSON                            1513\n",
      "LOITERING                        1225\n",
      "EMBEZZLEMENT                     1166\n",
      "SUICIDE                           508\n",
      "FAMILY OFFENSES                   491\n",
      "BAD CHECKS                        406\n",
      "BRIBERY                           289\n",
      "EXTORTION                         256\n",
      "SEX OFFENSES NON FORCIBLE         148\n",
      "GAMBLING                          146\n",
      "PORNOGRAPHY/OBSCENE MAT            22\n",
      "TREA                                6\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: Cumulative Sum of Counts for Top 5 Crimes\n",
      "Category\n",
      "LARCENY/THEFT     174900\n",
      "OTHER OFFENSES    301082\n",
      "NON-CRIMINAL      393386\n",
      "ASSAULT           470262\n",
      "DRUG/NARCOTIC     524233\n",
      "Name: Dates, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "category_count = train_data_full.groupby(['Category'])['Dates'].count()\n",
    "\n",
    "print (\"Training Data: Top 5 crimes\")\n",
    "print (category_count.sort_values(ascending=False)[:5])\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: Lowest 5 crimes\")\n",
    "print (category_count.sort_values(ascending=True)[:5])\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: All Category Counts\")\n",
    "print (category_count.sort_values(ascending=False))\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: Cumulative Sum of Counts for Top 5 Crimes\")\n",
    "print (category_count.sort_values(ascending=False)[:5].cumsum())\n",
    "\n",
    "#Top 5 crimes make up over 50% of the training data set.\n",
    "#Focusing on prediciting these as accurately as possible may give us a better accuracy score overall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting X and Y coordinates into San Francisco neighborhoods using Zillows database via shapefiles\n",
    "\n",
    "Shapefiles can be read via the pyshp package (https://pypi.python.org/pypi/pyshp)\n",
    "\n",
    "https://www.zillow.com/howto/api/neighborhood-boundaries.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the shapefile provided by zillow for california\n",
    "sf = shapefile.Reader(\"./Data/ZillowNeighborhoods-CA.shp\")\n",
    "fields = sf.fields[1:] \n",
    "field_names = [field[0] for field in fields] \n",
    "nhood_list = []\n",
    "\n",
    "#iterate through the shapefile records and retrieve the properties of each shapefile record \n",
    "#as well as its bbox coordinates \n",
    "#bbox: If the shape type contains multiple points this tuple describes the lower left (x,y) \n",
    "#coordinate and upper right corner coordinate creating a complete box around the points. \n",
    "#If the shapeType is a Null (shapeType == 0) then an AttributeError is raised.\n",
    "\n",
    "#we will use the bbox to determine if our X and Y coordinates from the training data\n",
    "#fall within the bbox of each neighborhood and then assign that neighborhood to the training data\n",
    "for r in sf.shapeRecords():  \n",
    "    atr = dict(zip(field_names, r.record))\n",
    "    bbox = r.shape.bbox\n",
    "    if atr['City'] == 'San Francisco':\n",
    "        #print (dict(properties=atr,bbox=bbox))\n",
    "        new_dict=dict(properties=atr,bbox=bbox)\n",
    "        nhood_list.append(new_dict)\n",
    "        \n",
    "def coord_in_bbox(bbox, X, Y):\n",
    "    if X>=bbox[0] and X<=bbox[2] and Y>=bbox[1] and Y<=bbox[3]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def neighborhood(nhood_list, X, Y):\n",
    "    for n in nhood_list:\n",
    "        bbox = n['bbox']\n",
    "        X = float(X)\n",
    "        Y = float(Y)\n",
    "        if coord_in_bbox(bbox, X, Y):\n",
    "            return n['properties']['Name']\n",
    "\n",
    "#Add neighborhood to train data\n",
    "train_data_full['Neighborhood'] = train_data_full.apply(lambda x: neighborhood(nhood_list,x['X'],x['Y']),axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Top 5 Criminal Neighborhoods\n",
      "Neighborhood\n",
      "South of Market    174414\n",
      "Downtown            90894\n",
      "Mission             83617\n",
      "South Beach         36298\n",
      "Bayview             32651\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: Lowest 5 Criminal Neighborhoods\n",
      "Neighborhood\n",
      "Monterey Heights     14\n",
      "Clarendon Heights    15\n",
      "Buena Vista Park     72\n",
      "Balboa Terrace       78\n",
      "Treasure Island      93\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: All Neighborhood Counts\n",
      "Neighborhood\n",
      "South of Market                             174414\n",
      "Downtown                                     90894\n",
      "Mission                                      83617\n",
      "South Beach                                  36298\n",
      "Bayview                                      32651\n",
      "Bernal Heights                               28829\n",
      "Hunters Point                                22562\n",
      "Hayes Valley                                 22519\n",
      "Western Addition                             21963\n",
      "Excelsior                                    21074\n",
      "Haight-Ashbury                               19484\n",
      "Portola                                      16848\n",
      "Potrero Hill                                 15952\n",
      "Eureka Valley - Dolores Heights - Castro     14082\n",
      "Financial District                           13193\n",
      "Visitacion Valley                            13030\n",
      "Cow Hollow                                   12783\n",
      "Pacific Heights                              12388\n",
      "Russian Hill                                 11653\n",
      "Chinatown                                    11442\n",
      "Central Richmond                             10088\n",
      "Nob Hill                                      9966\n",
      "Stonestown                                    9543\n",
      "North Beach                                   9273\n",
      "Inner Richmond                                8924\n",
      "Ingleside                                     8233\n",
      "Parkside                                      8112\n",
      "Inner Sunset                                  8078\n",
      "Outer Mission                                 7968\n",
      "Van Ness - Civic Center                       7260\n",
      "                                             ...  \n",
      "Forest Hill                                   1581\n",
      "Twin Peaks                                    1412\n",
      "Duboce Triangle                               1144\n",
      "Midtown Terrace                               1130\n",
      "Presidio Heights                              1099\n",
      "Forest Knolls                                 1044\n",
      "Miraloma Park                                 1028\n",
      "Jordan Park - Laurel Heights                   954\n",
      "Anza Vista                                     780\n",
      "Glen Park                                      732\n",
      "Ingleside Heights                              689\n",
      "Mission Terrace                                608\n",
      "Mission Bay                                    603\n",
      "Parnassus - Ashbury                            591\n",
      "Inner Parkside                                 439\n",
      "Lakeside                                       437\n",
      "Westwood Park                                  341\n",
      "Mount Davidson Manor                           329\n",
      "Merced Heights                                 315\n",
      "Corona Heights                                 299\n",
      "Sherwood Forest                                280\n",
      "St. Francis Wood                               244\n",
      "Golden Gate Heights                            213\n",
      "Sea Cliff                                      196\n",
      "Westwood Highlands                             194\n",
      "Treasure Island                                 93\n",
      "Balboa Terrace                                  78\n",
      "Buena Vista Park                                72\n",
      "Clarendon Heights                               15\n",
      "Monterey Heights                                14\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Training Data: Cumulative Sum of Counts for Top 5 Neighborhoods\n",
      "Neighborhood\n",
      "South of Market    174414\n",
      "Downtown           265308\n",
      "Mission            348925\n",
      "South Beach        385223\n",
      "Bayview            417874\n",
      "Name: Dates, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "neighborhood_count = train_data_full.groupby(['Neighborhood'])['Dates'].count()\n",
    "\n",
    "print (\"Training Data: Top 5 Criminal Neighborhoods\")\n",
    "print (neighborhood_count.sort_values(ascending=False)[:5])\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: Lowest 5 Criminal Neighborhoods\")\n",
    "print (neighborhood_count.sort_values(ascending=True)[:5])\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: All Neighborhood Counts\")\n",
    "print (neighborhood_count.sort_values(ascending=False))\n",
    "print (\"\")\n",
    "\n",
    "print (\"Training Data: Cumulative Sum of Counts for Top 5 Neighborhoods\")\n",
    "print (neighborhood_count.sort_values(ascending=False)[:5].cumsum())\n",
    "\n",
    "#Top 5 neighborhoods make up almost 50% of the training data set.\n",
    "#Also next steps is to bring in the category counts by neighborhood to see how these relate as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Date and Time Features\n",
    "Season  \n",
    "Month  \n",
    "Week  \n",
    "Day / Day of Week  \n",
    "Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert Dates to datetime\n",
    "train_data_full['FinalDate'] = pd.to_datetime(train_data_full['Dates'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create functions to extract the needed datetime features\n",
    "def season(date):\n",
    "    spring = range(80, 172)\n",
    "    summer = range(172, 264)\n",
    "    fall = range(264, 355)\n",
    "    if date.timetuple().tm_yday in spring:\n",
    "        return 2 #Spring\n",
    "    elif date.timetuple().tm_yday in summer:\n",
    "        return 3 #Summer\n",
    "    elif date.timetuple().tm_yday in fall:\n",
    "        return 4 #Fall\n",
    "    else:\n",
    "        return 1 #Winter\n",
    "    \n",
    "def getTimeCat(date):\n",
    "    # extract time categories\n",
    "    timecat = 4\n",
    "    ts =  datetime.strptime(str(date),  '%Y-%m-%d %H:%M:%S').time()\n",
    "\n",
    "    # --> Morning = 0400-1000\n",
    "    mornStart = datetime.strptime('2016-12-31 04:01',  '%Y-%m-%d %H:%M').time()\n",
    "    mornEnd = datetime.strptime('2016-12-31 10:00',  '%Y-%m-%d %H:%M').time()\n",
    "    \n",
    "    # --> Midday = 1000-1600\n",
    "    midStart = datetime.strptime('2016-12-31 10:01',  '%Y-%m-%d %H:%M').time()\n",
    "    midEnd = datetime.strptime('2016-12-31 16:00',  '%Y-%m-%d %H:%M').time()\n",
    "\n",
    "    # --> Evening = 1600-2300\n",
    "    eveStart = datetime.strptime('2016-12-31 16:01',  '%Y-%m-%d %H:%M').time()\n",
    "    eveEnd = datetime.strptime('2016-12-31 23:00',  '%Y-%m-%d %H:%M').time()\n",
    "\n",
    "    # --> Late Night = 2300-0400\n",
    "    lateStart1 = datetime.strptime('2016-12-31 23:01',  '%Y-%m-%d %H:%M').time()\n",
    "    lateEnd1 = datetime.strptime('2016-12-31 23:59',  '%Y-%m-%d %H:%M').time()\n",
    "    lateStart2 = datetime.strptime('2016-12-31 00:01',  '%Y-%m-%d %H:%M').time()\n",
    "    lateEnd2 = datetime.strptime('2016-12-31 04:01',  '%Y-%m-%d %H:%M').time()\n",
    "\n",
    "    \n",
    "    if ts >= mornStart and ts <= mornEnd:\n",
    "      timecat = 0 #morning\n",
    "    elif ts >= midStart and ts <= midEnd:\n",
    "      timecat = 1 #midday\n",
    "    elif ts >= eveStart and ts <= eveEnd:\n",
    "      timecat = 2 #evening\n",
    "    elif ts >= lateStart1 and ts <= lateEnd1:\n",
    "      timecat = 3 #late night\n",
    "    elif ts >= lateStart2 and ts <= lateEnd2:\n",
    "      timecat = 3 #late night\n",
    "\n",
    "    return timecat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apply the functions\n",
    "train_data_full['Season'] = train_data_full['FinalDate'].apply(lambda x: season(x))\n",
    "train_data_full['DayOfMonth'] = train_data_full['FinalDate'].apply(lambda x: x.day)\n",
    "train_data_full['Week'] = train_data_full['FinalDate'].apply(lambda x: x.week)\n",
    "train_data_full['TimeCat'] = train_data_full['FinalDate'].apply(lambda x: getTimeCat(x))\n",
    "train_data_full['Hour'] = train_data_full['FinalDate'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season Counts\n",
      "Season\n",
      "1    221171\n",
      "2    227590\n",
      "3    218856\n",
      "4    210432\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Day of Month Counts\n",
      "DayOfMonth\n",
      "1     32167\n",
      "2     27471\n",
      "3     28691\n",
      "4     29905\n",
      "5     29557\n",
      "6     29482\n",
      "7     29685\n",
      "8     30339\n",
      "9     29502\n",
      "10    28395\n",
      "11    27952\n",
      "12    28223\n",
      "13    28580\n",
      "14    27670\n",
      "15    28224\n",
      "16    28146\n",
      "17    29031\n",
      "18    29793\n",
      "19    30012\n",
      "20    29963\n",
      "21    30038\n",
      "22    30589\n",
      "23    29547\n",
      "24    27987\n",
      "25    26932\n",
      "26    26870\n",
      "27    27577\n",
      "28    27269\n",
      "29    27108\n",
      "30    26589\n",
      "31    14755\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Week # Counts\n",
      "Week\n",
      "2     35857\n",
      "4     35573\n",
      "6     35564\n",
      "8     35249\n",
      "10    36474\n",
      "12    35881\n",
      "14    35797\n",
      "16    35916\n",
      "18    35545\n",
      "20    34292\n",
      "22    32560\n",
      "24    31276\n",
      "26    32698\n",
      "28    32220\n",
      "30    32674\n",
      "32    32941\n",
      "34    34014\n",
      "36    33507\n",
      "38    33974\n",
      "40    34557\n",
      "42    33911\n",
      "44    34681\n",
      "46    32813\n",
      "48    30867\n",
      "50    30431\n",
      "52    28777\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Time Category Counts\n",
      "TimeCat\n",
      "0    132011\n",
      "1    266188\n",
      "2    341211\n",
      "3    138639\n",
      "Name: Dates, dtype: int64\n",
      "\n",
      "Hour Counts\n",
      "Hour\n",
      "0     44865\n",
      "1     26173\n",
      "2     22296\n",
      "3     14014\n",
      "4      9863\n",
      "5      8637\n",
      "6     13133\n",
      "7     22048\n",
      "8     32900\n",
      "9     35555\n",
      "10    37806\n",
      "11    38373\n",
      "12    51934\n",
      "13    43145\n",
      "14    44424\n",
      "15    48058\n",
      "16    50137\n",
      "17    53553\n",
      "18    55104\n",
      "19    49475\n",
      "20    44694\n",
      "21    43661\n",
      "22    45741\n",
      "23    42460\n",
      "Name: Dates, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#get basic counts of each new feature\n",
    "print(\"Season Counts\")\n",
    "print(train_data_full.groupby(['Season'])['Dates'].count())\n",
    "print(\"\")\n",
    "print(\"Day of Month Counts\")\n",
    "print(train_data_full.groupby(['DayOfMonth'])['Dates'].count())\n",
    "print(\"\")\n",
    "print(\"Week # Counts\")\n",
    "print(train_data_full.groupby(['Week'])['Dates'].count())\n",
    "print(\"\")\n",
    "print(\"Time Category Counts\")\n",
    "print(train_data_full.groupby(['TimeCat'])['Dates'].count())\n",
    "print(\"\")\n",
    "print(\"Hour Counts\")\n",
    "print(train_data_full.groupby(['Hour'])['Dates'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and Standardize Date and X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(series_field, df, new_field_name):\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    series_field = series_field.reshape((len(series_field), 1))\n",
    "    scaler = scaler.fit(series_field)\n",
    "    normalized = scaler.transform(series_field)\n",
    "    df[new_field_name] = normalized\n",
    "    return df\n",
    "\n",
    "def standardize(series_field, df, new_field_name):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    series_field = series_field.reshape((len(series_field), 1))\n",
    "    scaler = scaler.fit(series_field)\n",
    "    standardized = scaler.transform(series_field)\n",
    "    df[new_field_name] = standardized\n",
    "    return df\n",
    "\n",
    "train_data_full = normalize(train_data_full['FinalDate'], train_data_full, 'DateNorm')\n",
    "train_data_full = standardize(train_data_full['FinalDate'], train_data_full, 'DateStand')\n",
    "train_data_full = normalize(train_data_full['X'], train_data_full, 'XNorm')\n",
    "train_data_full = standardize(train_data_full['X'], train_data_full, 'XStand')\n",
    "train_data_full = normalize(train_data_full['Y'], train_data_full, 'YNorm')\n",
    "train_data_full = standardize(train_data_full['Y'], train_data_full, 'YStand')\n",
    "train_data_full = standardize(train_data_full['DateNorm'], train_data_full, 'DateNS')\n",
    "train_data_full = standardize(train_data_full['XNorm'], train_data_full, 'XNS')\n",
    "train_data_full = standardize(train_data_full['YNorm'], train_data_full, 'YNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dates                   object\n",
       "Category                object\n",
       "Descript                object\n",
       "DayOfWeek               object\n",
       "PdDistrict              object\n",
       "Resolution              object\n",
       "Address                 object\n",
       "X                       object\n",
       "Y                       object\n",
       "Neighborhood            object\n",
       "FinalDate       datetime64[ns]\n",
       "Season                   int64\n",
       "DayOfMonth               int64\n",
       "Week                     int64\n",
       "TimeCat                  int64\n",
       "Hour                     int64\n",
       "DateNorm               float64\n",
       "DateStand              float64\n",
       "XNorm                  float64\n",
       "XStand                 float64\n",
       "YNorm                  float64\n",
       "YStand                 float64\n",
       "DateNS                 float64\n",
       "XNS                    float64\n",
       "YNS                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a list of features and their data types\n",
    "train_data_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to matrix for model processing\n",
    "#Create sets of data where time and coords are standardized, scaled from [0,1], both\n",
    "X_full = train_data_full.as_matrix()\n",
    "Y = train_data_full[\"Category\"].as_matrix()\n",
    "X = train_data_full[[\"Dates\",\"X\",\"Y\"]].as_matrix()\n",
    "X_Norm = train_data_full[[\"DateNorm\",\"XNorm\",\"YNorm\"]].as_matrix()\n",
    "X_Std = train_data_full[[\"DateStand\",\"XStand\",\"YStand\"]].as_matrix()\n",
    "X_NS = train_data_full[[\"DateNS\",\"XNS\",\"YNS\"]].as_matrix()\n",
    "#Data set to investigate importance of time dimension\n",
    "X_2d = train_data_full[[\"XStand\",\"YStand\"]].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode categories into digits\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "n_classes = len(le.classes_)\n",
    "Y_le = le.transform(Y)\n",
    "\n",
    "#Shuffle the transformed data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X_full.shape[0]))\n",
    "X_Norm, X_Std, X_NS, X_2d, Y_le = X_Norm[shuffle],X_Std[shuffle],X_NS[shuffle],X_2d[shuffle],Y_le[shuffle]\n",
    "\n",
    "trn_N, trn_S, trn_NS, trn_2d, tr_labels = X_Norm[:len(X_full)//2], \\\n",
    "                                            X_Std[:len(X_full)//2], \\\n",
    "                                            X_NS[:len(X_full)//2], \\\n",
    "                                            X_2d[:len(X_full)//2], \\\n",
    "                                            Y_le[:len(X_full)//2] \n",
    "                \n",
    "tst_N, tst_S, tst_NS, tst_2d, tst_labels = X_Norm[len(X_full)//2:], \\\n",
    "                                            X_Std[len(X_full)//2:], \\\n",
    "                                            X_NS[len(X_full)//2:], \\\n",
    "                                            X_2d[len(X_full)//2:], \\\n",
    "                                            Y_le[len(X_full)//2:] \n",
    "trns = [trn_N, trn_S, trn_NS, trn_2d]\n",
    "tsts = [tst_N, tst_S, tst_NS, tst_2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: \n",
      "Accuracy: 0.040\n",
      "BIC: -7744920.307\n",
      "\n",
      "Model 2: \n",
      "Accuracy: 1.954\n",
      "BIC: 158154.880\n",
      "\n",
      "Model 3: \n",
      "Accuracy: 2.079\n",
      "BIC: 154076.075\n",
      "\n",
      "Model 4: \n",
      "Accuracy: 1.362\n",
      "BIC: -984784.985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize the GMM parameters by setting GMM means using labeled data.\n",
    "bics = []\n",
    "accs = []\n",
    "mod_names = [\"Normalized\",\"Standardized\", \"Normalized and Standardized\", \"2D Data Only\"]\n",
    "j=0\n",
    "\n",
    "#Fit GMM for each data set and report performance\n",
    "#Warning: Long Processing Time!\n",
    "for tr in trns:\n",
    "    j+=1\n",
    "    #Initialize the GMM with the average coordinates for each crime category\n",
    "    gm_means = np.array([tr[tr_labels == i].mean(axis=0) for i in range(n_classes)])\n",
    "    gm_mod = GaussianMixture(n_components = n_classes, covariance_type='full', means_init=gm_means)\n",
    "    gm_mod.fit(tr)\n",
    "    pred = gm_mod.predict(tr)\n",
    "    acc = np.mean(pred.ravel() == tr_labels.ravel()) * 100\n",
    "    accs.append(acc)\n",
    "    bic = gm_mod.bic(tr)\n",
    "    bics.append(bic)\n",
    "    print mod_names[j] + \" Model:\"\n",
    "    print \"Accuracy: %0.3f\" %acc\n",
    "    print \"BIC: %0.3f\\n\" %bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 3.087\n",
      "BIC: 18241.257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GMM With Normalization of 3 dimensions\n",
    "\n",
    "norm = preprocessing.Normalizer()\n",
    "#Apply after each dim has been standardized\n",
    "X_NS2 = norm.fit_transform(X_Std)\n",
    "print np.linalg.norm(X_NS2[1])\n",
    "\n",
    "#Encode categories into digits\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "n_classes = len(le.classes_)\n",
    "Y_le = le.transform(Y)\n",
    "\n",
    "#Previous model was lopsided.\n",
    "#Let's make a sample where each crime category is equally represented\n",
    "samps = []\n",
    "for i in range(n_classes):\n",
    "    cat_i_only = X_NS2[Y_le == i]\n",
    "    rands = np.random.randint(len(cat_i_only), size=1000)\n",
    "    samps.append(cat_i_only[rands])\n",
    "samps = np.array(samps)\n",
    "\n",
    "samp_labs = []\n",
    "for i in range(n_classes):\n",
    "    samp_labs.append([i]*1000)\n",
    "samp_labs = np.array(samp_labs).flatten()\n",
    "\n",
    "#Fit the GMM and Assess Performance\n",
    "bics = []\n",
    "accs = []\n",
    "samps_flat = samps.reshape(39000,3)\n",
    "gm_means = np.array([samps[i].mean(axis=0) for i in range(len(samps))])\n",
    "gm_mod = GaussianMixture(n_components = len(samps), covariance_type='full', means_init=gm_means)\n",
    "gm_mod.fit(samps_flat)\n",
    "pred = gm_mod.predict(samps_flat)\n",
    "acc = np.mean(pred.ravel() == samp_labs.ravel()) * 100\n",
    "accs.append(acc)\n",
    "bic = gm_mod.bic(samps_flat)\n",
    "bics.append(bic)\n",
    "print \"Accuracy: %0.3f\" %acc\n",
    "print \"BIC: %0.3f\\n\" %bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Warning: Long Processing Time\n",
    "#Let's try using GMM to create an additional feature\n",
    "#Find best GMM and use to create label column in X_full\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "comps = list(range(1,6))\n",
    "Xs_trn = [trn_N, trn_S, trn_NS]\n",
    "Xs_tst = [tst_N, tst_S, tst_NS]\n",
    "bic = []\n",
    "lowest_bic = np.infty\n",
    "for comp in comps:\n",
    "    for cv_type in cv_types:\n",
    "        for i in range(len(Xs_trn)):\n",
    "            gm_mod = GaussianMixture(n_components = comp, covariance_type=cv_type)\n",
    "            gm_mod.fit(Xs_trn[i])\n",
    "            bic.append(gm_mod.bic(Xs_trn[i]))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gm_mod\n",
    "                print \"Number of Comps: %d\" %comp\n",
    "                print \"CV Type: %s\" %cv_type\n",
    "                print \"i = %d\" %i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
